{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython\n",
      "Set autoreload\n"
     ]
    }
   ],
   "source": [
    "from neel.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "# huggingface_hub.HUGGINGFACE_CO_URL_HOME = \"https://huggingface.co/NeelNanda/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<huggingface_hub.repository.Repository object at 0x7fd69aa45e50>\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "from pathlib import Path\n",
    "repo_root = Path(\"/workspace/hf_repos\")\n",
    "repo = Repository(local_dir=repo_root/\"solu_test\")\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/NeelNanda/SoLU4/tree/main/test1/test2/test3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=repo_root/\"solu_test/test_checkpoint\",\n",
    "    path_in_repo=\"test1/test2/test3\",\n",
    "    repo_id=\"NeelNanda/SoLU4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21df2937026f4293b1544f50accd6cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/427M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1411477/4153173395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdownload_file_from_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SoLU\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"root\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1411477/4153173395.py\u001b[0m in \u001b[0;36mdownload_file_from_hf\u001b[0;34m(repo_name, file_name, subfolder)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                 \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                 \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                 cache_dir=CACHE_DIR)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             )\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;31m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "from pathlib import Path\n",
    "import torch\n",
    "CACHE_DIR = Path(\"/workspace/cache\")\n",
    "def download_file_from_hf(repo_name, file_name, subfolder=\".\"):\n",
    "    file_path = huggingface_hub.hf_hub_download(repo_id=f\"NeelNanda/{repo_name}\",\n",
    "                                                filename=file_name, \n",
    "                                                subfolder=subfolder, \n",
    "                                                cache_dir=CACHE_DIR)\n",
    "    try:\n",
    "        return torch.load(file_path)\n",
    "    except:\n",
    "        print(\"Could not load file as torch object\")\n",
    "        print(f\"Saved at file_path: {file_path}\")\n",
    "        return file_path\n",
    "    \n",
    "download_file_from_hf(\"SoLU\", \"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solu.utils import *\n",
    "state_dict = download_file_from_hf(\"SoLU\", \"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gree nn gregg ss gan dd gam'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"green greggs gand gam\"\n",
    "re.sub(r\"(\\w) \\w\", \" \\g<1>\\g<0>\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50258, 1024]) torch.Size([50278, 1024])\n",
      "pos_embed.W_pos torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.0.ln1.w not found\n",
      "blocks.0.ln1.b not found\n",
      "blocks.0.ln2.w not found\n",
      "blocks.0.ln2.b not found\n",
      "blocks.0.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.0.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.0.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.0.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.0.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.0.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.0.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.0.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.0.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.0.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.0.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.0.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.0.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.0.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.1.ln1.w not found\n",
      "blocks.1.ln1.b not found\n",
      "blocks.1.ln2.w not found\n",
      "blocks.1.ln2.b not found\n",
      "blocks.1.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.1.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.1.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.1.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.1.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.1.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.1.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.1.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.1.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.1.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.1.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.1.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.1.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.1.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.1.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.1.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.2.ln1.w not found\n",
      "blocks.2.ln1.b not found\n",
      "blocks.2.ln2.w not found\n",
      "blocks.2.ln2.b not found\n",
      "blocks.2.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.2.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.2.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.2.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.2.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.2.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.2.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.2.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.2.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.2.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.2.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.2.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.2.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.2.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.2.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.2.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.3.ln1.w not found\n",
      "blocks.3.ln1.b not found\n",
      "blocks.3.ln2.w not found\n",
      "blocks.3.ln2.b not found\n",
      "blocks.3.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.3.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.3.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.3.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.3.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.3.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.3.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.3.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.3.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.3.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.3.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.3.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.3.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.3.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.3.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.3.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.4.ln1.w not found\n",
      "blocks.4.ln1.b not found\n",
      "blocks.4.ln2.w not found\n",
      "blocks.4.ln2.b not found\n",
      "blocks.4.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.4.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.4.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.4.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.4.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.4.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.4.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.4.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.4.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.4.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.4.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.4.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.4.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.4.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.4.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.4.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.5.ln1.w not found\n",
      "blocks.5.ln1.b not found\n",
      "blocks.5.ln2.w not found\n",
      "blocks.5.ln2.b not found\n",
      "blocks.5.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.5.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.5.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.5.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.5.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.5.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.5.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.5.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.5.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.5.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.5.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.5.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.5.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.5.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.5.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.5.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.6.ln1.w not found\n",
      "blocks.6.ln1.b not found\n",
      "blocks.6.ln2.w not found\n",
      "blocks.6.ln2.b not found\n",
      "blocks.6.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.6.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.6.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.6.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.6.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.6.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.6.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.6.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.6.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.6.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.6.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.6.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.6.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.6.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.6.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.6.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "blocks.7.ln1.w not found\n",
      "blocks.7.ln1.b not found\n",
      "blocks.7.ln2.w not found\n",
      "blocks.7.ln2.b not found\n",
      "blocks.7.attn.W_Q torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.7.attn.W_K torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.7.attn.W_V torch.Size([16, 1024, 64]) torch.Size([16, 1024, 64])\n",
      "blocks.7.attn.W_O torch.Size([16, 64, 1024]) torch.Size([16, 64, 1024])\n",
      "blocks.7.attn.b_Q torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.7.attn.b_K torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.7.attn.b_V torch.Size([16, 64]) torch.Size([16, 64])\n",
      "blocks.7.attn.b_O torch.Size([1024]) torch.Size([1024])\n",
      "blocks.7.attn.mask torch.Size([1024, 1024]) torch.Size([1024, 1024])\n",
      "blocks.7.attn.IGNORE torch.Size([]) torch.Size([])\n",
      "blocks.7.mlp.W_in torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "blocks.7.mlp.b_in torch.Size([4096]) torch.Size([4096])\n",
      "blocks.7.mlp.W_out torch.Size([4096, 1024]) torch.Size([4096, 1024])\n",
      "blocks.7.mlp.b_out torch.Size([1024]) torch.Size([1024])\n",
      "blocks.7.mlp.ln.w torch.Size([4096]) torch.Size([4096])\n",
      "blocks.7.mlp.ln.b torch.Size([4096]) torch.Size([4096])\n",
      "ln_final.w not found\n",
      "ln_final.b not found\n",
      "unembed.W_U torch.Size([1024, 50258]) torch.Size([1024, 50278])\n",
      "unembed.b_U not found\n"
     ]
    }
   ],
   "source": [
    "for param in et.state_dict():\n",
    "    try:\n",
    "        print(param, et.state_dict()[param].shape, state_dict[param].shape)\n",
    "    except:\n",
    "        print(param, \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.ln2.b'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^norm\", r\".ln\", \"norm2.b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neel.imports import *\n",
    "et = EasyTransformer(dict(\n",
    "    n_layers=8,\n",
    "    d_model=1024,\n",
    "    n_ctx=1024,\n",
    "    d_mlp=4096,\n",
    "    d_head=64,\n",
    "    n_heads=16,\n",
    "    d_vocab=50278,\n",
    "    act_fn=\"solu_ln\",\n",
    "    normalization_type=\"LNPre\",\n",
    "    tokenizer_name='EleutherAI/gpt-neox-20b',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/workspace/solu_project/solu_checkpoints/v21_8L/SoLU_8L_v21_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in state_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping W_pos\n"
     ]
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    key = re.sub(r\"\\.norm\", r\".ln\", key)\n",
    "    key = re.sub(r\"^norm\", r\"ln_final\", key)\n",
    "    if key.endswith(\"W_pos\"):\n",
    "        print(\"Flipping W_pos\")\n",
    "        value = value.T\n",
    "    new_state_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_state_dict['unembed.b_U'] = torch.zeros_like(et.unembed.b_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.load_and_process_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4742, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et(\"Operation Hurricane was the first test of a British atomic device, detonated on 3 October 1952 in the lagoon in the Montebello Islands in Western Australia. During the Second World War, Britain commenced a nuclear weapons project, known as Tube Alloys, but the 1943 Quebec Agreement merged it with the American Manhattan Project. Several key British scientists worked on the Manhattan Project, but after the war the Americans ended cooperation. In January 1947, a cabinet sub-committee decided to resume efforts to build nuclear weapons. To test the effects of a ship-smuggled atomic bomb on a port (a threat of concern to the British at the time), the bomb was exploded inside the hull of a frigate, HMS Plym, leaving a saucer-shaped crater on the seabed 6 metres (20 ft) deep and 300 metres (1000 ft) across. With the success of Operation Hurricane, Britain became the third nuclear power, after the United States and the Soviet Union. (This article is part of a featured topic: Nuclear weapons and the United Kingdom.)\", return_type=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.7007, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({\"a\": 1}, open(\"/workspace/hf_repos/solu_test/test_checkpoint/test.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open(\"/workspace/hf_repos/solu_test/test_checkpoint/test.json\", \"r\"))['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/hf_repos/solu_test/stiff1.txt', '/workspace/hf_repos/solu_test/stiff2.txt']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rename: src should be string, bytes or os.PathLike, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1415578/550955900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"stiff*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"tiny_folder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: rename: src should be string, bytes or os.PathLike, not list"
     ]
    }
   ],
   "source": [
    "path = Path(\"/workspace/hf_repos/solu_test/\")\n",
    "import os\n",
    "from glob import glob\n",
    "x = glob(str(path/\"stiff*\"))\n",
    "print(x)\n",
    "os.rename(x, path/\"tiny_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_hub_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepo_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubfolder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepo_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlibrary_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlibrary_version\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muser_agent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforce_download\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforce_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mproxies\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0metag_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mresume_download\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_auth_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlegacy_cache_layout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Download a given file if it's not already present in the local cache.\n",
      "\n",
      "The new cache file layout looks like this:\n",
      "- The cache directory contains one subfolder per repo_id (namespaced by repo type)\n",
      "- inside each repo folder:\n",
      "    - refs is a list of the latest known revision => commit_hash pairs\n",
      "    - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on\n",
      "      whether they're LFS files or not)\n",
      "    - snapshots contains one subfolder per commit, each \"commit\" contains the subset of the files\n",
      "      that have been resolved at that particular commit. Each filename is a symlink to the blob\n",
      "      at that particular commit.\n",
      "\n",
      "[  96]  .\n",
      "└── [ 160]  models--julien-c--EsperBERTo-small\n",
      "    ├── [ 160]  blobs\n",
      "    │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n",
      "    │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e\n",
      "    │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812\n",
      "    ├── [  96]  refs\n",
      "    │   └── [  40]  main\n",
      "    └── [ 128]  snapshots\n",
      "        ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f\n",
      "        │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\n",
      "        │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n",
      "        └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48\n",
      "            ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\n",
      "            └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n",
      "\n",
      "Args:\n",
      "    repo_id (`str`):\n",
      "        A user or an organization name and a repo name separated by a `/`.\n",
      "    filename (`str`):\n",
      "        The name of the file in the repo.\n",
      "    subfolder (`str`, *optional*):\n",
      "        An optional value corresponding to a folder inside the model repo.\n",
      "    repo_type (`str`, *optional*):\n",
      "        Set to `\"dataset\"` or `\"space\"` if uploading to a dataset or space,\n",
      "        `None` or `\"model\"` if uploading to a model. Default is `None`.\n",
      "    revision (`str`, *optional*):\n",
      "        An optional Git revision id which can be a branch name, a tag, or a\n",
      "        commit hash.\n",
      "    library_name (`str`, *optional*):\n",
      "        The name of the library to which the object corresponds.\n",
      "    library_version (`str`, *optional*):\n",
      "        The version of the library.\n",
      "    cache_dir (`str`, `Path`, *optional*):\n",
      "        Path to the folder where cached files are stored.\n",
      "    user_agent (`dict`, `str`, *optional*):\n",
      "        The user-agent info in the form of a dictionary or a string.\n",
      "    force_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether the file should be downloaded even if it already exists in\n",
      "        the local cache.\n",
      "    proxies (`dict`, *optional*):\n",
      "        Dictionary mapping protocol to the URL of the proxy passed to\n",
      "        `requests.request`.\n",
      "    etag_timeout (`float`, *optional*, defaults to `10`):\n",
      "        When fetching ETag, how many seconds to wait for the server to send\n",
      "        data before giving up which is passed to `requests.request`.\n",
      "    resume_download (`bool`, *optional*, defaults to `False`):\n",
      "        If `True`, resume a previously interrupted download.\n",
      "    use_auth_token (`str`, `bool`, *optional*):\n",
      "        A token to be used for the download.\n",
      "            - If `True`, the token is read from the HuggingFace config\n",
      "              folder.\n",
      "            - If a string, it's used as the authentication token.\n",
      "    local_files_only (`bool`, *optional*, defaults to `False`):\n",
      "        If `True`, avoid downloading the file and return the path to the\n",
      "        local cached file if it exists.\n",
      "    legacy_cache_layout (`bool`, *optional*, defaults to `False`):\n",
      "        If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]\n",
      "        then `cached_download`. This is deprecated as the new cache layout is\n",
      "        more powerful.\n",
      "\n",
      "Returns:\n",
      "    Local path (string) of file or if networking is off, last version of\n",
      "    file cached on disk.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Raises the following errors:\n",
      "\n",
      "    - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)\n",
      "      if `use_auth_token=True` and the token cannot be found.\n",
      "    - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)\n",
      "      if ETag cannot be determined.\n",
      "    - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\n",
      "      if some parameter value is invalid\n",
      "    - [`~huggingface_hub.utils.RepositoryNotFoundError`]\n",
      "      If the repository to download from cannot be found. This may be because it doesn't exist,\n",
      "      or because it is set to `private` and you do not have access.\n",
      "    - [`~huggingface_hub.utils.RevisionNotFoundError`]\n",
      "      If the revision to download from cannot be found.\n",
      "    - [`~huggingface_hub.utils.EntryNotFoundError`]\n",
      "      If the file to download cannot be found.\n",
      "    - [`~huggingface_hub.utils.LocalEntryNotFoundError`]\n",
      "      If network is disabled or unavailable and file is not found in cache.\n",
      "\n",
      "</Tip>\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.7/site-packages/huggingface_hub/file_download.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "huggingface_hub.hf_hub_download?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBinaryIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath_in_repo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepo_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepo_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0midentical_ok\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcommit_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcommit_description\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcreate_pr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparent_commit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Upload a local file (up to 50 GB) to the given repo. The upload is done\n",
      "through a HTTP post request, and doesn't require git or git-lfs to be\n",
      "installed.\n",
      "\n",
      "Args:\n",
      "    path_or_fileobj (`str`, `bytes`, or `IO`):\n",
      "        Path to a file on the local machine or binary data stream /\n",
      "        fileobj / buffer.\n",
      "    path_in_repo (`str`):\n",
      "        Relative filepath in the repo, for example:\n",
      "        `\"checkpoints/1fec34a/weights.bin\"`\n",
      "    repo_id (`str`):\n",
      "        The repository to which the file will be uploaded, for example:\n",
      "        `\"username/custom_transformers\"`\n",
      "    token (`str`, *optional*):\n",
      "        Authentication token, obtained with `HfApi.login` method. Will\n",
      "        default to the stored token.\n",
      "    repo_type (`str`, *optional*):\n",
      "        Set to `\"dataset\"` or `\"space\"` if uploading to a dataset or\n",
      "        space, `None` or `\"model\"` if uploading to a model. Default is\n",
      "        `None`.\n",
      "    revision (`str`, *optional*):\n",
      "        The git revision to commit from. Defaults to the head of the\n",
      "        `\"main\"` branch.\n",
      "    identical_ok (`bool`, *optional*, defaults to `True`):\n",
      "        Deprecated: will be removed in 0.11.0.\n",
      "        Changing this value has no effect.\n",
      "    commit_message (`str`, *optional*):\n",
      "        The summary / title / first line of the generated commit\n",
      "    commit_description (`str` *optional*)\n",
      "        The description of the generated commit\n",
      "    create_pr (`boolean`, *optional*):\n",
      "        Whether or not to create a Pull Request from `revision` with that commit.\n",
      "        Defaults to `False`.\n",
      "    parent_commit (`str`, *optional*):\n",
      "        The OID / SHA of the parent commit, as a hexadecimal string. Shorthands (7 first characters) are also supported.\n",
      "        If specified and `create_pr` is `False`, the commit will fail if `revision` does not point to `parent_commit`.\n",
      "        If specified and `create_pr` is `True`, the pull request will be created from `parent_commit`.\n",
      "        Specifying `parent_commit` ensures the repo has not changed before committing the changes, and can be\n",
      "        especially useful if the repo is updated / committed to concurrently.\n",
      "\n",
      "\n",
      "Returns:\n",
      "    `str`: The URL to visualize the uploaded file on the hub\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Raises the following errors:\n",
      "\n",
      "    - [`HTTPError`](https://2.python-requests.org/en/master/api/#requests.HTTPError)\n",
      "      if the HuggingFace API returned an error\n",
      "    - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\n",
      "      if some parameter value is invalid\n",
      "    - [`~huggingface_hub.utils.RepositoryNotFoundError`]\n",
      "      If the repository to download from cannot be found. This may be because it doesn't exist,\n",
      "      or because it is set to `private` and you do not have access.\n",
      "    - [`~huggingface_hub.utils.RevisionNotFoundError`]\n",
      "      If the revision to download from cannot be found.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import upload_file\n",
      "\n",
      ">>> with open(\"./local/filepath\", \"rb\") as fobj:\n",
      "...     upload_file(\n",
      "...         path_or_fileobj=fileobj,\n",
      "...         path_in_repo=\"remote/file/path.h5\",\n",
      "...         repo_id=\"username/my-dataset\",\n",
      "...         repo_type=\"datasets\",\n",
      "...         token=\"my_token\",\n",
      "...     )\n",
      "\"https://huggingface.co/datasets/username/my-dataset/blob/main/remote/file/path.h5\"\n",
      "\n",
      ">>> upload_file(\n",
      "...     path_or_fileobj=\".\\\\local\\\\file\\\\path\",\n",
      "...     path_in_repo=\"remote/file/path.h5\",\n",
      "...     repo_id=\"username/my-model\",\n",
      "...     token=\"my_token\",\n",
      "... )\n",
      "\"https://huggingface.co/username/my-model/blob/main/remote/file/path.h5\"\n",
      "\n",
      ">>> upload_file(\n",
      "...     path_or_fileobj=\".\\\\local\\\\file\\\\path\",\n",
      "...     path_in_repo=\"remote/file/path.h5\",\n",
      "...     repo_id=\"username/my-model\",\n",
      "...     token=\"my_token\",\n",
      "...     create_pr=True,\n",
      "... )\n",
      "\"https://huggingface.co/username/my-model/blob/refs%2Fpr%2F1/remote/file/path.h5\"\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "api = huggingface_hub.HfApi()\n",
    "api.upload_file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspace/hf_repos')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPO_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
